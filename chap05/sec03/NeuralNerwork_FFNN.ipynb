{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class neuralNetwork:\n",
    "    '''ニューラルネットワークを生成する\n",
    "    \n",
    "    Attributes:\n",
    "      input(int)(int) : 入力層のニューロン数\n",
    "      fc1(int): 隠れ層のニューロン数\n",
    "      fc2(int): 出力層のニューロン数\n",
    "      lr(float): 学習率\n",
    "      w1(array): 隠れ層のバイアス、重み行列\n",
    "      w2(array): 出力層のバイアス、重み行列\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate):\n",
    "        '''ニューラルネットワークの初期化を行う\n",
    "        \n",
    "        Parameters:\n",
    "          input_dim(int) : 入力層のニューロン数\n",
    "          hidden_dim(int): 隠れ層のニューロン数\n",
    "          output_dim(int): 出力層のニューロン数\n",
    "          learning_rate(float): 学習率\n",
    "\n",
    "        '''\n",
    "        # 入力層、隠れ層、出力層のニューロン数をインスタンス変数に代入\n",
    "        self.input = input_dim    # 入力層のニューロン数\n",
    "        self.fc1 = hidden_dim     # 隠れ層のニューロン数\n",
    "        self.fc2 = output_dim     # 出力層のニューロン数\n",
    "        self.lr = learning_rate   # 学習率\n",
    "        self.weight_initializer() # weight_initializer()を呼ぶ\n",
    "\n",
    "    def weight_initializer(self):\n",
    "        '''重みとバイアスの初期化を行う\n",
    "        \n",
    "        '''\n",
    "        # 隠れ層の重みとバイアス\n",
    "        # (隠れ層のニューロン数\n",
    "        self.w1 = np.random.normal(\n",
    "            0.0,                   # 平均は0\n",
    "            pow(self.input, -0.5), # 標準偏差は入力層のデータサイズを元に計算\n",
    "            (self.fc1,             # 行数は隠れ層のニューロン数\n",
    "             self.input + 1)       # 列数は入力層のデータサイズ + 1\n",
    "            )\n",
    "        \n",
    "       # 出力層の重みとバイアスを初期化\n",
    "        self.w2 = np.random.normal(\n",
    "            0.0,                 # 平均は0\n",
    "            pow(self.fc1, -0.5), # 標準偏差は隠れ層のニューロン数を元に計算\n",
    "            (self.fc2,           # 行数は出力層のニューロン数\n",
    "             self.fc1 + 1)       # 列数は隠れ層のニューロン数 + 1\n",
    "            )\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''シグモイド関数\n",
    "        \n",
    "        Parameters:\n",
    "          x(array): 関数を適用するデータ\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''ソフトマックス関数\n",
    "        \n",
    "        Parameters:\n",
    "          x(array): 関数を適用するデータ\n",
    "        '''\n",
    "        c = np.max(x)\n",
    "        exp_x = np.exp(x - c) # オーバーフロー対策\n",
    "        sum_exp_x = np.sum(exp_x)\n",
    "        y = exp_x / sum_exp_x\n",
    "        return y\n",
    "    \n",
    "    def train(self, train_x, train_y):\n",
    "        '''ニューラルネットワークの学習を行う\n",
    "        \n",
    "        Parameters：\n",
    "          train_x(array): 訓練データ\n",
    "          train_y(array): 正解ラベル\n",
    "        '''\n",
    "        ## [入力層]\n",
    "        # 入力値の配列にバイアス項を追加して入力層から出力する\n",
    "        inputs = np.array(\n",
    "            np.append(train_x, [1]), # 配列の末尾にバイアスのための「1」を追加\n",
    "            ndmin=2                  # 2次元化\n",
    "            ).T                      # 転置して1列の行列にする\n",
    "\n",
    "        ## [隠れ層]\n",
    "        # 入力層の出力に重み、バイアスを適用して隠れ層に入力する\n",
    "        hidden_inputs = np.dot(\n",
    "            self.w1,              # 隠れ層の重み\n",
    "            inputs                # 入力層の出力\n",
    "            )\n",
    "        # シグモイド関数を適用して隠れ層から出力\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)        \n",
    "        # 隠れ層の出力行列の末尾にバイアスのための「1」を追加\n",
    "        hidden_outputs = np.append(\n",
    "            hidden_outputs,      # 隠れ層の出力行列\n",
    "            [[1]],               # 2次元形式でバイアス値を追加\n",
    "            axis=0               # 行を指定(列は1)\n",
    "            )\n",
    "        \n",
    "        ## [出力層]\n",
    "        # 出力層への入力信号を作る\n",
    "        final_inputs = np.dot(\n",
    "            self.w2,             # 隠れ層と出力層の間の重み\n",
    "            hidden_outputs       # 隠れ層の出力\n",
    "            )\n",
    "        # ソフトマックス関数を適用して出力層から出力する\n",
    "        final_outputs = self.softmax(final_inputs)\n",
    "        \n",
    "        print(final_outputs)      # 一時的に追加するコード、実験終了後に削除\n",
    "        print(sum(final_outputs)) # 一時的に追加するコード、実験終了後に削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21544986]\n",
      " [0.11554453]\n",
      " [0.66900561]]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# FFNNの挙動を見るためのコード\n",
    "# 確認後、削除する\n",
    "input_dim = 3  # 入力データのサイズ\n",
    "hidden_dim = 3 # 隠れ層のニューロンの数\n",
    "output_dim = 3 # 出力層のニューロンの数\n",
    "learning_rate = 0.1 # 学習率\n",
    "\n",
    "# neuralNetworkオブジェクトの生成\n",
    "n = neuralNetwork(input_dim, hidden_dim, output_dim, learning_rate)\n",
    "\n",
    "# ダミーの訓練データ\n",
    "inputs_list = [1.0, 1.5, 2.0]\n",
    "# ダミーの正解ラベル\n",
    "targets_list = [1.0, 1.5, 2.0]\n",
    "#train()を実行\n",
    "n.train(inputs_list, targets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
